{
 "cells": [
  {
   "cell_type": "raw",
   "id": "indoor-costa",
   "metadata": {},
   "source": [
    "18.2.1 Steps for Preparing Data\n",
    "\n",
    "Before moving data to our unsupervised algorithms, complete the following steps for preparing data:\n",
    "\n",
    "1. Data selection\n",
    "2. Data processing\n",
    "3. Data transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "extraordinary-water",
   "metadata": {},
   "source": [
    "Data Selection\n",
    "\n",
    "Data selection entails making good choices about which data will be used. Consider what data is available, what data is missing, and what data can be removed. For example, say we have a dataset on city weather that consists of temperature, population, latitude and longitude, date, snowfall, and income. After looking through the columns, we can readily see that population and income data don't affect weather. We might also notice some rows are missing temperature data. In the data selection process, we would remove the population and income columns as well as any rows that don't record temperatures."
   ]
  },
  {
   "cell_type": "raw",
   "id": "southern-shannon",
   "metadata": {},
   "source": [
    "Data Processing\n",
    "\n",
    "Data processing involves organizing the data by formatting, cleaning, and sampling it. In our dataset on city weather, if the date column has two different formats—mm-dd-yyyy (e.g., 01-23-1980) and month-data-year (e.g., jan-23-1980)—we would convert all dates to the same format."
   ]
  },
  {
   "cell_type": "raw",
   "id": "interpreted-shell",
   "metadata": {},
   "source": [
    "Data Transformation\n",
    "\n",
    "Data transformation entails transforming our data into a simpler format for storage and future use, such as a CSV, spreadsheet, or database file. Once our weather data is cleaned and processed, we would export the final version of the data as a CSV file for future analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "smooth-divide",
   "metadata": {},
   "source": [
    "18.2.2 Pandas Refresher\n",
    "\n",
    "Pandas is a Python library that is excellent for data munging. We'll be using the iris dataset from the UCI Machine Learning Repository, a common dataset used throughout machine learning:\n",
    "\n",
    "1. Store the raw iris.csv \n",
    "\n",
    "2. Open a new Jupyter Notebook.\n",
    "\n",
    "3. Import your libraries:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "4. To load the dataset in a Pandas DataFrame, enter the code below. Be sure to use the path to the stored CSV file (stored in an easy-to-access location):\n",
    "\n",
    "file_path = \"<folder path to stored data sets>/iris.csv\"\n",
    "iris_df = pd.read_csv(file_path)\n",
    "iris_df.head()\n",
    "\n",
    "5. Select the fields of data you want:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "stock-tackle",
   "metadata": {},
   "source": [
    "Which field should we drop from the DataFrame?\n",
    "Class - Unsupervised learning only deals with numerical data, so this column can be dropped.\n",
    "\n",
    "Unsupervised learning will be used to determine the class of the iris plants later on in the module.\n",
    "\n",
    "6. Drop the class field using the code below:\n",
    "new_iris_df = iris_df.drop(['class'], axis=1)\n",
    "new_iris_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "concerned-rainbow",
   "metadata": {},
   "source": [
    "SKILL DRILL\n",
    "\n",
    "Try reordering the columns so the sepal and petal lengths are the first two columns and the widths are the last two columns.\n",
    "\n",
    "new_iris_df = new_iris_df[['sepal_length', 'petal_length', 'sepal_width', 'petal_width']]\n",
    "new_iris_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "english-promise",
   "metadata": {},
   "source": [
    "Cleaning this dataset appears complete with all the data in numerical form and the same type, so no data processing is needed. \n",
    "\n",
    "However, you'll encounter data transformations on datasets that contain categorical data or non-numeric features (e.g., transforming male and female categorical values to 0 and 1, respectively).\n",
    "\n",
    "Finally, the preprocessed DataFrame is saved on a new CSV file for future use. This is done by storing the file path in a variable, then using the Pandas to_csv() method to export the DataFrame to a CSV by supplying the file path and file name as arguments, as shown below:\n",
    "\n",
    "output_file_path = \"<path to folder>/new_iris_data.csv\"\n",
    "new_iris_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hungarian-dinner",
   "metadata": {},
   "source": [
    "18.2.3 Preprocessing Data With Pandas\n",
    "\n",
    "As mentioned, we don't know the output of the data, but that doesn't mean we shouldn't think about our data or that we should carelessly plug it into a model.\n",
    "\n",
    "Let's take a look at how we should start our data processing by loading in the shopping_data.csv\n",
    "\n",
    "# Load data\n",
    "file_path = \"Resources/shopping_data.csv\"\n",
    "df_shopping = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "df_shopping.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "under-validity",
   "metadata": {},
   "source": [
    "Questions for Data Preparation\n",
    "\n",
    "Unsupervised learning doesn't have a clear outcome or target variable like supervised learning, but it is used to find patterns. By properly preparing the data, we can select features that help us find patterns or groups.\n",
    "\n",
    "Before we begin, consider these questions:\n",
    "\n",
    "What knowledge do we hope to glean from running an unsupervised learning model on this dataset?\n",
    "What data is available? What type? What is missing? What can be removed?\n",
    "Is the data in a format that can be passed into an unsupervised learning model?\n",
    "Can I quickly hand off this data for others to use?\n",
    "\n",
    "\n",
    "Let's address the first question on our list:\n",
    "\n",
    "What knowledge do we hope to glean from running an unsupervised learning model on this dataset?\n",
    "\n",
    "It's a shopping dataset, so we can group together shoppers based on spending habits."
   ]
  },
  {
   "cell_type": "raw",
   "id": "thorough-contamination",
   "metadata": {},
   "source": [
    "18.2.4 Data Selection\n",
    "\n",
    "To convince an accounting firm to invest in cryptocurrency, we want to make sure you know how to select the data that will best help the model determine patterns or grouping.\n",
    "\n",
    "To help us select the data, let's return to some of the questions on our list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "offshore-think",
   "metadata": {},
   "source": [
    "What data is available?\n",
    "\n",
    "First, account for the data you have. After all, you can't extract knowledge without data. We can use the columns method and output the columns, as shown below:\n",
    "\n",
    "# Columns\n",
    "df_shopping.columns\n",
    "\n",
    "Looking at the columns, we see there is data for CustomerID, Age, Annual Income, and Spending Score:\n",
    "\n",
    "Now that we know what data we have, we can start thinking about possible analysis. \n",
    "\n",
    "For example, data points for features like Age and Annual Income might appear in our end result as groupings or clusters. However, there are no data points for items purchased, so our algorithms cannot discover related patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dressed-maine",
   "metadata": {},
   "source": [
    "What type of data is available?\n",
    "\n",
    "Using the dtypes method, confirm the data type, which also will alert us if anything should be changed in the next step (e.g., converting text to numerical data). All the columns we plan to use in our model must contain a numerical data type:\n",
    "\n",
    "# List dataframe datatypes\n",
    "df_shopping.dtypes\n",
    "\n",
    "CustomerID                  int64\n",
    "Card Member                object\n",
    "Age                       float64\n",
    "Annual Income               int64\n",
    "Spending Score (1-100)    float64\n",
    "dtype: object"
   ]
  },
  {
   "cell_type": "raw",
   "id": "moving-bullet",
   "metadata": {},
   "source": [
    "Which column doesn’t contain a data type we can use for our unsupervised learning model?\n",
    "Card Member - This column contains a data type object, which is not numerical."
   ]
  },
  {
   "cell_type": "raw",
   "id": "unnecessary-architecture",
   "metadata": {},
   "source": [
    "What data is missing?\n",
    "\n",
    "Next, let's see if any data is missing. Unsupervised learning models can't handle missing data. If you try to run a model on a dataset with missing data, you'll get an error such as the one below:\n",
    "\n",
    "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
    "\n",
    "If you initially had hoped to produce an outcome using a type of data, but it turned out more than 80% of those rows are empty, then the results won't be very accurate!\n",
    "\n",
    "For example, return to our Age and Income groups: If it turns out there are 1,200 rows without any Age data points, then we clearly can't use that column in our model. There is no set cutoff for missing data—that decision is left up to you, the analyst, and must be made based on your understanding of the business needs.\n",
    "\n",
    "Handling missing data is a complex topic that is out of scope for this unit. However, if you're interested, read this article (Links to an external site.) on the possible approaches to handling missing data.\n",
    "\n",
    "Pandas has the isnull() method to check for missing values. We'll loop through each column, check if there are null values, sum them up, and print out a readable total:\n",
    "\n",
    "There will be a few rows with missing values that we'll need to handle. The judgement call will be to either remove these rows or decide that the dataset is not suitable for our model. In this case, we'll proceed with handling these values because they are a small percentage of the overall data.\n",
    "\n",
    "When deciding to proceed, the percentage of data missing isn't always the only determining factor. See the Note callout above for a resource on handling missing data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "prerequisite-convenience",
   "metadata": {},
   "source": [
    "# What data is missing?\n",
    "# Find null values\n",
    "\n",
    "for column in df_shopping.columns: \n",
    "    print(f\"Column {column} has {df_shopping[column].isnull().sum()}null values\")\n",
    "\n",
    "Column CustomerID has 0null values\n",
    "Column Card Member has 2null values\n",
    "Column Age has 2null values\n",
    "Column Annual Income has 0null values\n",
    "Column Spending Score (1-100) has 1null values\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "moderate-canada",
   "metadata": {},
   "source": [
    "What data can be removed?\n",
    "\n",
    "You have begun to explore the data and have taken a look at null values. Next, determine if the data can be removed. Consider: Are there string columns that we can't use? Are there columns with excessive null data points? Was our decision to handle missing values to just remove them?\n",
    "\n",
    "In our example, there are no string type columns, and we made the decision that only a few rows have null data points, but not enough to remove a whole column's worth.\n",
    "\n",
    "Rows of data with null values can be removed with the dropna() method, as shown below:\n",
    "\n",
    "# Drop null rows\n",
    "df_shopping = df_shopping.dropna()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "incorporated-minority",
   "metadata": {},
   "source": [
    "Duplicates can also be removed.\n",
    "Duplicates aren’t telling us anything new.\n",
    "Duplicates could skew our results.\n",
    "\n",
    "Use the duplicated().sum() method to check for duplicates, as shown below:\n",
    "\n",
    "# Find duplicate entries\n",
    "print(f\"Duplicate entries: {df_shopping.duplicated().sum()}\")\n",
    "\n",
    "Duplicate entries: 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "changed-partner",
   "metadata": {},
   "source": [
    "We also can remove data that doesn’t tell us anything interesting. Knowing this, is there anything we can remove from this DataFrame?\n",
    "\n",
    "CustomerID - does not offer any insight into customer shopping habits.\n",
    "\n",
    "To remove the column, just enter the code below:\n",
    "\n",
    "# Remove the CustomerID Column\n",
    "df_shopping.drop(columns=[\"CustomerID\"], inplace=True)\n",
    "df_shopping.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "involved-fiber",
   "metadata": {},
   "source": [
    "18.2.5 Data Processing\n",
    "\n",
    "Now that you know what kind of data you want to work, it's time to meet the needs for your unsupervised model.\n",
    "\n",
    "The next step is to move on from what you (the user) want to get out of your data and on to what the unsupervised model needs out of the data.\n",
    "\n",
    "Recall that in the data selection step, you, as the user, are exploring the data to see what kind of insights and analysis you might glean. You reviewed the columns available and the data types stored, and determined if there were missing values."
   ]
  },
  {
   "cell_type": "raw",
   "id": "spiritual-basement",
   "metadata": {},
   "source": [
    "For data processing, the focus is on making sure the data is set up for the unsupervised learning model, which requires the following:\n",
    "\n",
    "- Null values are handled.\n",
    "\n",
    "- Only numerical data is used.\n",
    "\n",
    "- Values are scaled. In other words, data has been manipulated to ensure that the variance between the numbers won't skew results.\n",
    "\n",
    "Recall that when features have different scales, they can have a disproportionate impact on the model. The unscaled value could lead to messy graphs. Therefore, it is important to understand when to scale and normalize data.\n",
    "\n",
    "For example, if four columns of data are single digits, and the fifth column is in the millions, we would need to scale the fifth column to align the other four."
   ]
  },
  {
   "cell_type": "raw",
   "id": "compact-idaho",
   "metadata": {},
   "source": [
    "Let's return again to our list of questions.\n",
    "\n",
    "Is the data in a format that can be passed into an unsupervised learning model?\n",
    "\n",
    "We saw before that all our data had the correct type for each column; however, we know that our model can't have strings passed into it.\n",
    "\n",
    "To make sure we can use our string data, we'll transform our strings of Yes and No from the Card Member column to 1 and 0, respectively, by creating a function that will convert Yes to a 1 and anything else to 0.\n",
    "\n",
    "The function will then be run on the whole column with the .apply method, as shown below:\n",
    "\n",
    "# Transform String Column\n",
    "\n",
    "def change_string(member):\n",
    "    if member == \"Yes\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_shopping[\"Card Member\"] = df_shopping[\"Card Member\"].apply(change_string)\n",
    "df_shopping.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "expressed-capacity",
   "metadata": {},
   "source": [
    "Also, there is one more thing you may notice about the data. The scale for Annual Income is much larger than all the other values in the dataset. We can adjust this format by dividing by 1,000 to rescale those data points, as shown below:\n",
    "\n",
    "# Transform annual income to rescale data points \n",
    "df_shopping[\"Annual Income\"] = df_shopping[\"Annual Income\"] / 1000\n",
    "df_shopping.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "shaped-healing",
   "metadata": {},
   "source": [
    "SKILL DRILL\n",
    "Reformat the names of the columns so they contain no spaces or numbers.\n",
    "\n",
    "df_shopping.rename(columns={'Card Member': 'Card_Member', 'Annual Income': 'Annual_Income', 'Spending Score (1-100)': 'Spending_Score'}, inplace=True)\n",
    "df_shopping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-cleaner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
