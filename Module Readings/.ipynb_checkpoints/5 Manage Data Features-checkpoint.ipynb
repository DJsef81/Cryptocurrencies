{
 "cells": [
  {
   "cell_type": "raw",
   "id": "excess-stereo",
   "metadata": {},
   "source": [
    "18.5.1 Dimensionality Reduction\n",
    "\n",
    "Martha has noticed that so far we have been working with pretty good datasets in terms of data used. Even after some data cleanup, there haven't been too many features to work with. However, she is beginning to worry that her cryptocurrency data has too many features and is not sure how this will affect our model. The way to handle this is with dimensionality reduction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "eight-serum",
   "metadata": {},
   "source": [
    "Think back to our example with the store owner who is trying to sell school supplies. His customer data could contain endless features, or columns. \n",
    "\n",
    "The data could include name, age, address, items bought, amount spent, time spent shopping, zip code, and so forth. Some features just aren't necessary and could throw off our algorithm. For instance, would converting names to an integer value be worth the time or even inform our analysis?\n",
    "\n",
    "Also, throwing all of these features into the model might overfit the data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "centered-antarctica",
   "metadata": {},
   "source": [
    "Why is overfitting a model bad?\n",
    "\n",
    "It makes the model too specific and unable to work with additional datasets.\n",
    "\n",
    "If your model is too specific, future datasets that have different trends will be less accurate.\n",
    "\n",
    "Since overfitting is bad, it is best to find a way to limit features. The process of reducing features is called dimensionality reduction. There are two options for coping with too many features: elimination and extraction."
   ]
  },
  {
   "cell_type": "raw",
   "id": "sensitive-princess",
   "metadata": {},
   "source": [
    "Feature Elimination\n",
    "\n",
    "Your first idea is to remove a good amount of features so the model won't be run using every column. This is called feature elimination.\n",
    "\n",
    "Feature elimination means what you think: You remove, or eliminate, a feature from the dataset. In our school supply example, you remove features that aren't relevant to what we're looking for, such as name, address, and zip code. This simple method increases and maintains interpretability.\n",
    "\n",
    "The downside is, once you remove that feature, you can no longer glean information from it. If we want to know the likelihood of people buying school supplies, but we removed the zip code feature, then we'd miss a detail that could help us understand when certain residents tend to purchase school supplies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "mexican-department",
   "metadata": {},
   "source": [
    "Feature Extraction\n",
    "\n",
    "Feature extraction combines all features into a new set that is ordered by how well they predict our original variable.\n",
    "\n",
    "In other words, feature extraction reduces the number of dimensions by transforming a large set of variables into a smaller one. This smaller set of variables contains most of the important information from the original large set.\n",
    "\n",
    "NOTE\n",
    "Sometimes, you need to use both feature elimination and extraction. For instance, the customer name feature doesn't inform us about whether or not customers will purchase school supplies. So, we would eliminate that feature during the preprocessing stage, then apply extraction on the remaining features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "reserved-pharmacy",
   "metadata": {},
   "source": [
    "18.5.2 Principal Component Analysis\n",
    "\n",
    "Your client assured you that all the data they have collected is important and needs to be used. Being worried about overfitting your data, you decided to use Principal Component Analysis (PCA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "liquid-firewall",
   "metadata": {},
   "source": [
    "PCA is a statistical technique to speed up machine learning algorithms when the number of input features (or dimensions) is too high. PCA reduces the number of dimensions by transforming a large set of variables into a smaller one that contains most of the information in the original large set.\n",
    "\n",
    "PCA is a complicated process to understand, but it is easy to code. Let's start out by coding some PCA into our K-means, so that you can see it in action, then revisit the code's underlying theory."
   ]
  },
  {
   "cell_type": "raw",
   "id": "taken-birthday",
   "metadata": {},
   "source": [
    "Using the new_iris_data.csv (Links to an external site.) first, import the libraries weâ€™ll use and load the data into a Pandas DataFrame:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "raw",
   "id": "frozen-donna",
   "metadata": {},
   "source": [
    "# Loading the preporcessed iris dataset \n",
    "file_path = \"../Resources/new_iris_data.csv\"\n",
    "df_iris = pd.read_csv(file_path)\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "presidential-fence",
   "metadata": {},
   "source": [
    "There are four features in this dataset with values on different scales. The first step in PCA is to standardize these features by using the StandardScaler library:\n",
    "\n",
    "# Standadize data with StandardScaler\n",
    "iris_scaled = StandardScaler().fit_transform(df_iris)\n",
    "print(iris_scaled[0:5])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "given-final",
   "metadata": {},
   "source": [
    "Now that the data has been standardized, we can use PCA to reduce the number of features. The PCA method takes an argument of n_components, which will pass in the value of 2, thus reducing the features from 4 to 2:\n",
    "\n",
    "# Initialize PCA model\n",
    "pca = PCA(n_components=2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "alpine-tonight",
   "metadata": {},
   "source": [
    "After creating the PCA model, we apply dimensionality reduction on the scaled dataset:\n",
    "\n",
    "# Get two principal components for the iris data.\n",
    "iris_pca = pca.fit_transform(iris_scaled)\n",
    "\n",
    "After this dimensionality reduction, we get a smaller set of dimensions called principal components. These new components are just the two main dimensions of variation that contain most of the information in the original dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "stupid-polyester",
   "metadata": {},
   "source": [
    "The resulting principal components are transformed into a DataFrame to fit K-means:\n",
    "\n",
    "# Transform PCA data to a dataframe\n",
    "\n",
    "df_iris_pca = pd.DataFrame(\n",
    "    data=iris_pca, columns=[\"principal component 1\", \"principal component 2\"]\n",
    ")\n",
    "df_iris_pca.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "polyphonic-devices",
   "metadata": {},
   "source": [
    "# Use explained_variance_ratio to learn how much information can be attributed to each principal component:\n",
    "# Fetch the explained variance\n",
    "\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "array([0.72770452, 0.23030523])\n",
    "\n",
    "What this tells us, is that the first principal component contains 72.77% of the variance and the second contains 23.03%. Together, they contain 95.80% of the information."
   ]
  },
  {
   "cell_type": "raw",
   "id": "forward-blond",
   "metadata": {},
   "source": [
    "Next, we'll use the elbow curve with the generated principal components and see the K value is 3:\n",
    "\n",
    "# Find the best value for K\n",
    "inertia = []\n",
    "k = list(range(1, 11))\n",
    "\n",
    "# Calculate the inertia for the range of K values\n",
    "for i in k:\n",
    "    km = KMeans(n_clusters=i, random_state=0)\n",
    "    km.fit(df_iris_pca)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "# Create the elbow curve\n",
    "elbow_data = {\"k\": k, \"inertia\": inertia}\n",
    "df_elbow = pd.DataFrame(elbow_data)\n",
    "df_elbow.hvplot.line(x=\"k\", y=\"inertia\", xticks=k, title=\"Elbow Curve\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "secure-chassis",
   "metadata": {},
   "source": [
    "Use the principal components data with the K-means algorithm with a K value of 3. We could consider 2, but the direction shifts more after 3:\n",
    "\n",
    "# Initialize the K-means model\n",
    "model = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(df_iris_pca)\n",
    "\n",
    "# Predict clusters\n",
    "predictions = model.predict(df_iris_pca)\n",
    "\n",
    "# Add the predicted class columns\n",
    "df_iris_pca[\"class\"] = model.labels_\n",
    "df_iris_pca.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "disabled-miniature",
   "metadata": {},
   "source": [
    "Finally, we can plot the clusters. Instead of a 3D plot, the data is easier to analyze with only two features:\n",
    "\n",
    "df_iris_pca.hvplot.scatter(\n",
    "    x=\"principal component 1\",\n",
    "    y=\"principal component 2\",\n",
    "    hover_cols=[\"class\"],\n",
    "    by=\"class\",\n",
    ")\n",
    "\n",
    "The next few sections will go over exactly how PCA works and can be a bit daunting. Remember, you can already code it!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "going-memory",
   "metadata": {},
   "source": [
    "18.5.3 Mean, Variance, and Covariance\n",
    "\n",
    "Now that you convinced Martha that feature extraction is the way to go, she needs some background on why this works in case questions come up during her presentation on how she can \"magically\" combine these features in a meaningful way. To start, you dust off your stats knowledge and refresh your memory on mean, variance, and covariance. These will be the building blocks used for PCA.\n",
    "\n",
    "There is a mathematical way to use feature extraction, but first let's review some stats concepts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "velvet-martial",
   "metadata": {},
   "source": [
    "Mean\n",
    "\n",
    "Recall that the mean is the sum of a group of numbers divided by the total amount of numbers. For example, we start with points 2, 3, and 7. \n",
    "\n",
    "First, we add up all the numbers: 2 + 3 + 7 = 12. Then we divide the result by the total amount of points, which is 3. So, 12 / 3 = 4, so the mean of those three points is 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "interior-thailand",
   "metadata": {},
   "source": [
    "Variance\n",
    "\n",
    "Variance is the square distance from each point from the center, added together, and divided by the total number of points. The variance measures the spread of a set of numbers. \n",
    "\n",
    "The center of the points may look familiar, and it should, because it is the mean of all the points. Variance, in other words, is a measure of how far apart the data points are from the mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "boring-luther",
   "metadata": {},
   "source": [
    "DATA POINTS: -4, 0, 4\n",
    "\n",
    "Using 0 as the center point, the distances are -4 from the center, 0 from the center (the center point is still a point), and 4 from the center.\n",
    "\n",
    "The sum of squared distances would be (-4)^2 + (0)^2 + (4)^2 = 16 + 0\n",
    "\n",
    "16 = 32. We use squared distance so they are all positive.\n",
    "\n",
    "Divide by the total number of points, which is 3. The variance of this dataset would be 32/3, or 10â…”.\n",
    "\n",
    "Normally, there won't be an even distribution of points around the center. The points 2, 3, and 7 from the previous example don't have a clear center.\n",
    "\n",
    "This is where the mean comes into play. The center of the line is set to the mean, which we found to be 4. Here is what the points look like on a line:\n",
    "\n",
    "2, 3, 4, 7"
   ]
  },
  {
   "cell_type": "raw",
   "id": "blond-saskatchewan",
   "metadata": {},
   "source": [
    "The distance from 4 to 2 is -2, the distance from 4 to 3 is -1, the distance from 4 to 4 is 0, and the distance from 4 to 7 is 3.\n",
    "\n",
    "Add up the squares of each distance: (-2)^2 + (-1)^2 + (0)^2 + (3)^2 = 4 + 1 + 0 + 9 = 14.\n",
    "\n",
    "Finally, divide the distances by the total number of points: 14 / 3. The variance equals 14/3, or 4â…”.\n",
    "\n",
    "These examples showed points on the x-axis, and thus, form the x variance. The same process applies to elements on the y-axis, forming the y variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "handmade-likelihood",
   "metadata": {},
   "source": [
    "Covariance\n",
    "\n",
    "Before defining what covariance is, look at the following two plots:\n",
    "\n",
    "PLOT 1 \n",
    "1, 3 : 2,2 : 3,1 \n",
    "\n",
    "PLOT 2 \n",
    "1,1: 2, 2: 3, 3\n",
    "\n",
    "These two plots clearly are very different. Each has the same center, with different points on the left and the right, one sloping negatively and the other sloping positively."
   ]
  },
  {
   "cell_type": "raw",
   "id": "vocal-exhaust",
   "metadata": {},
   "source": [
    "Let's find the x and y variance for each line.\n",
    "\n",
    "For graph A:\n",
    "\n",
    "The center point is (2, 2).\n",
    "The distances for the points are the distance from (2, 2).\n",
    "Point (1, 3) is a distance of -1 away on the x-axis and 1 on the y-axis.\n",
    "Point (3, 1) is a distance of 1 away on the x-axis and -1 on the y-axis.\n",
    "x variance = (-1)^2 + 0^2 + (1)^2 = 2 / 3\n",
    "y variance = (1)^2 + 0^2 + (-1)^2 = 2 / 3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "operational-geography",
   "metadata": {},
   "source": [
    "For graph B:\n",
    "\n",
    "The center point is (2, 2).\n",
    "The distances for the points are the distance from (2, 2).\n",
    "Point (1, 1) is a distance of -1 away on the x-axis and -1 on the y-axis.\n",
    "Point (3, 3) is a distance of 1 away on the x-axis and 1 on the y-axis.\n",
    "x variance = (-1)^2 + 0^2 + (1)^2 = 2 / 3\n",
    "y variance = (-1)^2 + 0^2 + (1)^2 = 2 / 3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "balanced-legislation",
   "metadata": {},
   "source": [
    "Wait. Both of these variances are exactly the same; however, it is very obvious that these two graphs are totally different! How can we tell the difference?\n",
    "\n",
    "This is where covariance comes into play. Covariance is a metric that allows us to tell these two different sets of points apart.\n",
    "\n",
    "How can we tell the difference between the points that lie along Line A (-3,-1 : 0,0 : 3,1)  versus the points that lie along Line B (-3,1 : 0,0 : 3, -1)?\n",
    "\n",
    "We can do this with the product of coordinates, which is the multiple of each of the two points:\n",
    "\n",
    "Line A (-3,-1 : 0,0 : 3,1) = (-3, 0, 3)\n",
    "\n",
    "Line B (-3,1 : 0,0 : 3, -1) = (-3, 0, -3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "focal-space",
   "metadata": {},
   "source": [
    "Covariance is the sum of the product of coordinates divided by the number of points.\n",
    "\n",
    "Covariance is used to determine the relationship between points.\n",
    "\n",
    "The formula for covariance is as follows:\n",
    "\n",
    "LaTeX: Cov\\left(x,y\\right)= \\frac{ \\sum\\left(x_i-\\bar{x}\\right) (y_i-\\bar{y})}{\\mathbb{N} }C o v ( x , y ) = âˆ‘ ( x i âˆ’ x Â¯ ) ( y i âˆ’ y Â¯ ) N\n",
    "\n",
    "What this equation is saying is that the covariance takes the sum of the product between each pair of coordinates and their difference from the mean divided by the total number of points. This may sound complicated but will make more sense once we look at an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bound-boulder",
   "metadata": {},
   "source": [
    "Let's solve for the covariance of line A first which contains the points (-3, -1), (0, 0) and (3, 1).\n",
    "\n",
    "First take the mean of the x coordinates in line A, -3 + 0 + 3 = 0 divided by 3 is zero. (mean = x with line above)\n",
    "Then repeat for the y coordinates, -1 + 0 + 1 = 0 divided by 3 is also zero. (mean = y with line above)\n",
    "\n",
    "Then for each pair of coordinates find the difference between the point and their respective means.\n",
    "\n",
    "X\t Y\t      \n",
    "-3\t-1\t    -3 - 0 = -3\t  -1 - 0 = -1 \n",
    "0\t 0\t     0 - 0 = 0\t   0 - 0 = 0\n",
    "3\t 1\t     3 - 0 = 3\t   1 - 0 = 1\n",
    "\n",
    "Now multiply the results of the coordinate pairs\n",
    "\n",
    "-3 x -1 = 3\n",
    "3 x 1 = 3\n",
    "\n",
    "Finally add the product of all the coordinated paris and divide by the number of points to find the covariance.\n",
    "\n",
    "3 + 0 + 3 = 6\n",
    "\n",
    "Plug the results into the top part of the equation, and since we know there or 3 points, we plug that in for N to get.\n",
    "\n",
    "Cov(x,y)= 6/3\n",
    "\n",
    "Reduce the equation.\n",
    "Cov(x,y) = 2\n",
    "\n",
    "The covariance for line A is 2\n",
    "\n",
    "Same equation applied to line B\n",
    "\n",
    "The covariance for line A is -2.\n",
    "\n",
    "We can then say that Line A has a positive covariance (at 2) while Line B has a negative covariance (at -2). There is also a third type of covariance called covariance zero. This is when the points tend to form a horizontal line.\n",
    "\n",
    "Covariance is used to only describe the relationship between points, such as positive and negative as we just saw. You may recall another method for determining relationships is correlation. However, correlation is used to determine the strength of the relationship."
   ]
  },
  {
   "cell_type": "raw",
   "id": "extended-singer",
   "metadata": {},
   "source": [
    "18.5.4 Linear Transformations\n",
    "\n",
    "Martha appreciates the refreshers on stats but is wondering where this is going. Well, patience is a virtue, and trust that all of this forms the building blocks to really start to understand how PCA works. Next up is linear transformations."
   ]
  },
  {
   "cell_type": "raw",
   "id": "concerned-notion",
   "metadata": {},
   "source": [
    "Say we have a set of points on a graph. We want to center these points by taking the average of the coordinate, both X and Y. Find the balance point and move that to zero:\n",
    "\n",
    "Once the points are centered, we're going to create a 2x2 matrix that consists of the variance and covariances that we found in the previous step:\n",
    "\n",
    "variance (x)       covariance (x,y)\n",
    "covariance (x,y)   variance (y)\n",
    "\n",
    "So, let's say the matrix above contains the following:\n",
    "6 2\n",
    "2 3\n",
    "\n",
    "This matrix will be used to transform the points from one graph to another by using the numbers to create a formula for our transformation. The top two values of the matrix will correspond to one point and the bottom two values to another."
   ]
  },
  {
   "cell_type": "raw",
   "id": "demographic-clark",
   "metadata": {},
   "source": [
    "In our example, the formula for the points becomes (6x + 2y, 2x + 3y). Let's plug some coordinates into the formula:\n",
    "\n",
    "(x, y)\t(6x + 2y, 2x + 3y)\n",
    "(0,0)\t(0,0)\n",
    "(1,0)\t(6,2)\n",
    "(0,1)\t(2,3)\n",
    "(-1,0)\t(-6,-2)\n",
    "(0,-1)\t(-2,-3)\n",
    "\n",
    "Now, let's plot the new points from the right side of the matrix to create a linear transformation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "substantial-directive",
   "metadata": {},
   "source": [
    "Eigenvectors and Eigenvalues\n",
    "\n",
    "NOTE\n",
    "Eigenvectors and eigenvalues can be complicated subjects rooted in linear algebra. We cover these at a very high level, but if you wish to explore more on your own, you can read more about Eigenvalues and eigenvectors (Links to an external site.) and watch this video\n",
    "\n",
    "As you can see, the points stretch out in our graph in two directions. One direction moves from southwest to northeast direction while another direction moves from southeast to northwest. These are called eigenvectors\n",
    "\n",
    "There is a way to figure out the vectors and values with algebra, but we use the calculator on WolframAlpha (Links to an external site.) to simplify the process. Plug in our matrix of {{6,2},{2,3}}, then click calculate.\n",
    "\n",
    "From the results website, you can see in one direction the shape stretched to a value of 7 and another to a value of 2. The magnitude that each of these stretches is called the eigenvalue:\n",
    "\n",
    "We also see the direction that stretched with the eigenvectors of (2, 1) and (-1, 2):\n",
    "\n",
    "The big takeaway from eigenvectors and eigenvalues is that they show us the spread of the dataset and by how much."
   ]
  },
  {
   "cell_type": "raw",
   "id": "czech-audience",
   "metadata": {},
   "source": [
    "18.5.5 PCA's Underlying Theory\n",
    "\n",
    "Now it's time to put everything together and show how PCA works. Given our two eigenvalues from before, 7 and 2, take the greater eigenvalue, 7, and eliminate the other since it's less important. The higher eigenvalue is the axis that carries the most amount of information.\n",
    "\n",
    "We'll also take the corresponding eigenvector, which is (2,1).\n",
    "\n",
    "Next, extend that eigenvector with the higher value to a line and project all our points onto that line:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "quality-sunrise",
   "metadata": {},
   "source": [
    "Now let's put everything together and show what PCA is doing. We'll up the ante a little bit and expand from two to five columns of data. First, take our data that consists of five columns, or features. Note, the asterisk (*) will represent a number as we'll avoid using numbers to simplify the exercise:\n",
    "\n",
    "x1\tx2\tx3\tx4\tx5\n",
    "*\t*\t*\t*\t*\n",
    "*\t*\t*\t*\t*\n",
    "*\t*\t*\t*\t*\n",
    "\n",
    "Put all the data points into a 5x5 covariance matrix. The eigenvectors and eigenvalues are calculated for each of those five columns in the matrix. Again, the asterisk (*) represents a number:\n",
    "\n",
    "From the matrix we can produce a list of eigenvectors and corresponding eigenvalues:\n",
    "V1 A1\n",
    "V2 A2 \n",
    "V3 A3\n",
    "V4 A4\n",
    "V5 A5\n",
    "\n",
    "We pick how many eigenvalues we want to keep and which to drop. For this example, we'll keep the top two eigenvalues and drop three:\n",
    "V1 A1\n",
    "V2 A2\n",
    "\n",
    "Taking two will allow us to plot on a 2D plane. The two eigenvalues and eigenvectors will create a plane on which all the points can be plotted:\n",
    "\n",
    "This now narrows down our five features to two and gives us a good snapshot of what the data should look like because we chose the directions the data spread the most.\n",
    "\n",
    "Finally, these data points will give us a table of two columns, where the asterisk (*) is a number. Remember, when we coded PCA, the end result was two columns of principal components:\n",
    "\n",
    "Principal Component 1\tPrincipal Component 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "popular-frost",
   "metadata": {},
   "source": [
    "The statistics, linear transformations, and eigenvalues and eigenvectors all illustrate how PCA works. As you saw earlier, it is much easier to code than do all of this math. So, don't worry if this is confusingâ€”remember, you've already coded it! It is important to understand, on some level, what PCA is doing in case you're ever asked in an interview.\n",
    "\n",
    "That wraps up how PCA works. (Wow! That was a lot of fancy jargon!) Thankfully, code has made our work easier and now you have a better understanding of how to reduce dimensions yet still keep the values.\n",
    "\n",
    "Next, look at another form of clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
