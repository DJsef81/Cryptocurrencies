{
 "cells": [
  {
   "cell_type": "raw",
   "id": "limited-compact",
   "metadata": {},
   "source": [
    "18.6.1 Understanding Hierarchical Clustering\n",
    "\n",
    "You both covered a lot of ground today with unsupervised learning, and Martha now feels ready to complete her project for her client. She does have one last question: Is K-means the only sort of clustering algorithm? \n",
    "\n",
    "There is another type, hierarchical clustering."
   ]
  },
  {
   "cell_type": "raw",
   "id": "hairy-production",
   "metadata": {},
   "source": [
    "Similar to K-means clustering, hierarchical clustering, also known as agglomerative clustering, works with groups (clusters) of data points. The algorithm starts by declaring each point with its own cluster, then merges the two most similar clusters until a declared stopping point has been reached. This stopping point is implemented within your code.\n",
    "\n",
    "Hierarchical clustering has three methods for determining how points are linked: ward, average, and complete."
   ]
  },
  {
   "cell_type": "raw",
   "id": "amber-marketplace",
   "metadata": {},
   "source": [
    "Ward is the algorithm's default setting. Simply put, this function selects the two clusters that, when merged, will mean the least amount of variance between all remaining clusters. This often leads to clusters that are relatively equal in size."
   ]
  },
  {
   "cell_type": "raw",
   "id": "saved-water",
   "metadata": {},
   "source": [
    "You can also link points using the average. This method connects clusters that have the smallest average distance between all of their points, then connects clusters on the smallest average distance between all their points."
   ]
  },
  {
   "cell_type": "raw",
   "id": "plain-track",
   "metadata": {},
   "source": [
    "The third method is complete, which links by merging clusters that have the smallest maximum distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "curious-houston",
   "metadata": {},
   "source": [
    "For example, say we're given the following points and our stopping point is set at two clusters:\n",
    "\n",
    "Each point, at the start, is considered a part of its own cluster, so we can say our starting point is eight clusters. We'll start by grouping together the two closest points:\n",
    "\n",
    "We now have seven clusters, and our stopping point of two clusters has not been met, so we keep going. The next two closest points are then grouped together, and we continue with this process:\n",
    "\n",
    "Now, the next closest points contain a point in a cluster. When this happens, we join that point to the closest clusters. Remember, each point is considered a cluster:\n",
    "\n",
    "The next two closest points to each other are now both contained in a cluster, since the stopping point of two clusters has still not been met, these clusters are merged:\n",
    "\n",
    "We have now reached the stopping point of two clusters, and we just completed hierarchical clustering.\n",
    "\n",
    "Now, you might wonder how we determine the ideal amount of clusters, as we did for the elbow curve. We'll cover that in the next section with dendrograms."
   ]
  },
  {
   "cell_type": "raw",
   "id": "innovative-orange",
   "metadata": {},
   "source": [
    "18.6.2 Dendrograms\n",
    "\n",
    "Just like with K-means, trial and error for the amount of clusters is not ideal, even in hierarchical clustering. Luckily, there is a similar method with the use of dendrograms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "focused-secretary",
   "metadata": {},
   "source": [
    "A dendrogram is a graph that keeps the values of the points on the x-axis, then connects all the points as they are clustered. This is similar to the elbow curve, as it gives us a better idea of the ideal amount of clusters we want to use. After making a dendrogram, you'll know how many clusters to make based on how refined you want them to be.\n",
    "\n",
    "Let's build on the clusters from the prior exercise to create our first dendrogram:\n",
    "\n",
    "Each point on the axis represents our initial point. Next, we'll start connecting the points with a tree for each cluster that is joined:\n",
    "\n",
    "We now have three clusters, with two points unaccounted for, and the points that were merged into a cluster will be connected in the dendrogram:\n",
    "\n",
    "Next, combine the next merger of two clusters:\n",
    "\n",
    "In the previous example, we didn't need to merge all points into one cluster. As you'll see, we'll join the remaining two clusters into one and plot it on the dendrogram:\n",
    "\n",
    "Now we can set the height to determine how many clusters we want:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "classical-consideration",
   "metadata": {},
   "source": [
    "Choose the tolerance of cluster distance and draw a line at that point. Note how many clusters are beneath that line.\n",
    "\n",
    "The solid horizontal lines indicate how far apart the clusters are from each other before they're merged. \n",
    "\n",
    "The dashed line for three clusters indicates that the longest horizontal lines below it must be that far away from each other to form three clusters. \n",
    "\n",
    "The dashed line for two clusters indicates that the top horizontal lines are at a greater distance from three clusters to two clusters. \n",
    "\n",
    "Finally, the horizontal line above the dashed line for two clusters would be an even greater distance if we were to form one cluster.\n",
    "\n",
    "Therefore, the greater the distance, the less similarity exists. There is no correct choice between two and three here. Again, that decision is left up to you, the analyst, to decide how refined you want the clusters to be.\n",
    "\n",
    "In the next section, we'll return to the iris dataset and see how running hierarchical clustering works."
   ]
  },
  {
   "cell_type": "raw",
   "id": "duplicate-gambling",
   "metadata": {},
   "source": [
    "18.6.3 Running Hierarchical Clustering\n",
    "\n",
    "The two of you are curious to see what hierarchical clustering has to offer over K-means and decide to test it out on the iris dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "generic-deposit",
   "metadata": {},
   "source": [
    "Open a notebook and enter the following code to import our libraries. (Most of these should look familiar by this point, with the only new one being the AgglomerativeClustering library, the hierarchical clustering algorithm that will replace K-means.):\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "raw",
   "id": "average-sigma",
   "metadata": {},
   "source": [
    "Enter the code to load in the iris dataset:\n",
    "\n",
    "# load data \n",
    "file = \"../Resources/new_iris_data.csv\"\n",
    "df_iris = pd.read_csv(file)\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "lucky-financing",
   "metadata": {},
   "source": [
    "SKILL DRILL\n",
    "\n",
    "Apply PCA to reduce the dataset from four features to two.\n",
    "\n",
    "# Standadize data with StandardScaler\n",
    "iris_scaled = StandardScaler().fit_transform(df_iris)\n",
    "print(iris_scaled[0:5])\n",
    "\n",
    "[[-0.90068117 -1.3412724   1.03205722 -1.31297673]\n",
    " [-1.14301691 -1.3412724  -0.1249576  -1.31297673]\n",
    " [-1.38535265 -1.39813811  0.33784833 -1.31297673]\n",
    " [-1.50652052 -1.2844067   0.10644536 -1.31297673]\n",
    " [-1.02184904 -1.3412724   1.26346019 -1.31297673]]\n",
    " \n",
    " # Initialize PCA model (Reduce number of features from 4 to 2)\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# apply dimensionality reduction on the scaled dataset:\n",
    "# Get two principal components for the iris data.\n",
    "# After this dimensionality reduction, we get a smaller set of dimensions called principal components. \n",
    "# These new components are just the two main dimensions of variation that contain most of the information in the \n",
    "# original dataset.\n",
    "\n",
    "\n",
    "iris_pca = pca.fit_transform(iris_scaled)\n",
    "\n",
    "# The resulting principal components are transformed into a DataFrame to fit K-means:\n",
    "# Transform PCA data to a dataframe\n",
    "\n",
    "df_iris_pca = pd.DataFrame(\n",
    "    data=iris_pca, columns=[\"principal component 1\", \"principal component 2\"]\n",
    ")\n",
    "df_iris_pca.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "informational-citation",
   "metadata": {},
   "source": [
    "After PCA has been applied, it is time to run the hierarchical clustering algorithm. We start by creating a dendrogram. Enter the code to import the libraries:\n",
    "\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "raw",
   "id": "tracked-pierce",
   "metadata": {},
   "source": [
    "Enter the code to create a dendrogram. We'll pass a color_threshold of 0 to make all the branches the same color:\n",
    "\n",
    "# Create the dendrogram\n",
    "fig = ff.create_dendrogram(df_iris_pca, color_threshold=0)\n",
    "fig.update_layout(width=800, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "polished-oasis",
   "metadata": {},
   "source": [
    "Now it is up to us to determine how many clusters we want to make. Remember, the higher the horizontal lines, the less similarity there is between the clusters. We know the iris dataset contains three clusters. The cutoff will be set at five to obtain three clusters:\n",
    "\n",
    "We knew ahead of time the number of clusters to make; however, the cutoff line on the dendrogram seems high in terms of distances. This is one of the difficulties when using a dendrogram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wrong-tactics",
   "metadata": {},
   "source": [
    "Now it's time to run the hierarchical algorithm. Agglomerative clustering is another name for hierarchical clustering. Enter the following code:\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "model = agg.fit(df_iris_pca)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "innocent-producer",
   "metadata": {},
   "source": [
    "This will set up our model, and since you're working with a dataset that you're already familiar with, there should be three clustered groups we decided previously, so three will be passed into the n_clusters parameter. Then the model is fit against your df_iris_pcaDataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ethical-manitoba",
   "metadata": {},
   "source": [
    "Next, add a class column that will be used to identify the clusters:\n",
    "\n",
    "# Add a new class column to df_iris\n",
    "df_iris_pca[\"class\"] = model.labels_\n",
    "df_iris_pca.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fantastic-techno",
   "metadata": {},
   "source": [
    "# Finally, create a plot to show the results of the hierarchical clustering algorithm:\n",
    "df_iris_pca.hvplot.scatter(\n",
    "    x=\"principal component 1\",\n",
    "    y=\"principal component 2\",\n",
    "    hover_cols=[\"class\"],\n",
    "    by=\"class\",\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "compact-insurance",
   "metadata": {},
   "source": [
    "You'll see that the process is similar for both types of clustering algorithms, and so are the results. You decide whether to apply the K-means or hierarchical clustering algorithm. In the next section, we'll review the pros and cons of each clustering algorithm.\n",
    "\n",
    "SKILL DRILL\n",
    "Re-run the algorithm using different cutoffs from the dendrogram."
   ]
  },
  {
   "cell_type": "raw",
   "id": "israeli-hours",
   "metadata": {},
   "source": [
    "18.6.4 K-means vs. Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering seems like a fairly interesting idea, but you wonder what the differences are between K-means and hierarchical."
   ]
  },
  {
   "cell_type": "raw",
   "id": "advisory-scheme",
   "metadata": {},
   "source": [
    "The K-means algorithm is the main algorithm we used in this module. It is easy, runs relatively quickly, and can scale to large datasets. This is not to say there aren't drawbacks to the K-means algorithm.\n",
    "\n",
    "Behind the scenes, K-means is dependent on random initialization, so the outcome depends on a random seed. \n",
    "\n",
    "With K-means, you need to have an idea of how many clusters you're looking for ahead of time, which might not always be known. \n",
    "\n",
    "This can be an issue when the points of data are not so clearly grouped into clusters, as K-means works best for spherical-looking data with similar density points closely grouped together.\n",
    "\n",
    "With hierarchical clustering and the use of dendrograms, it's easier to pick how many clusters we want without making any assumptions since a K value does not need to be known ahead of time.\n",
    "\n",
    "The dendrogram might not always create as clear of a choice as we would like, and it leaves the final decision up to the analyst. \n",
    "\n",
    "With the iris dataset, we knew the K value ahead of time, so using K-means in that situation would make more sense. \n",
    "\n",
    "Hierarchical clustering might not work as well on larger datasets because it is slower at run time, and there are a lot of decisions to be made about when to merge groups of clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
